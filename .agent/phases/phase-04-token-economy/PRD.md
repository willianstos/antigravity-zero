# PRD: Phase 04 - Token Economy Engine (Grok 2026) ðŸ’°

//full-auto

---

## 1. DiagnÃ³stico do Problema

O Open Claw Bot Ã© um **buraco de tokens** porque:

1. **System prompt gordo**: O `PERSONA.md` + todos os memos do `.context/` sÃ£o injetados em CADA mensagem. Isso pode ser 3.000â€“8.000 tokens por chamada.
2. **Sem roteamento de modelo**: Toda pergunta, simples ou complexa, vai para o modelo mais caro (GPT-4o / Grok).
3. **Sem cache semÃ¢ntico**: Perguntas similares repetem a chamada completa Ã  API.
4. **Loop recursivo sem limite real**: O `while (depth < MAX_DEPTH)` pode fazer 3 chamadas completas por mensagem.
5. **Contexto Qdrant nÃ£o filtrado**: `getFullAwareness()` injeta tudo que encontra, sem threshold de relevÃ¢ncia.

**Estimativa de custo atual**: ~15.000 tokens/mensagem complexa Ã— $5/M tokens (Grok) = **$0,075/mensagem**. Em 100 mensagens/dia = **$7,50/dia = $225/mÃªs**.

**Meta pÃ³s-otimizaÃ§Ã£o**: < $0,005/mensagem = **< $15/mÃªs**.

---

## 2. Arquitetura Token Economy (PadrÃ£o 2026)

```
[Mensagem Telegram]
       â†“
[Classifier: simples/mÃ©dio/complexo] â† modelo local (gratuito)
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SIMPLES â†’ Grok Mini / Flash (barato) â”‚
â”‚ MÃ‰DIO   â†’ Grok 3 / GPT-4o-mini      â”‚
â”‚ COMPLEXOâ†’ Grok 4 / GPT-4o (full)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
[Semantic Cache Redis] â†’ HIT? â†’ retorna sem chamar API
       â†“ MISS
[Context Triangulation] â†’ injeta APENAS contexto relevante (RAG Qdrant)
       â†“
[LLM Call] â†’ resposta
       â†“
[Cache Store Redis TTL=1h]
```

---

## 3. Roles & Skills
- `mestre-qdrant`: RAG com threshold de relevÃ¢ncia.
- `guardiao-de-secrets`: Redis como cache semÃ¢ntico.
- `zelador-do-codigo`: Refatorar `openclaw-bridge.mjs` e `context-manager.mjs`.

---

## 4. Fila de Tasks

- [ ] **T01**: Criar `src/jarvis/ai/token-router.mjs` â€” roteador de modelo por complexidade.
- [ ] **T02**: Criar `src/jarvis/ai/semantic-cache.mjs` â€” cache Redis para respostas similares.
- [ ] **T03**: Refatorar `getFullAwareness()` no bridge â€” aplicar threshold de relevÃ¢ncia no Qdrant.
- [ ] **T04**: Comprimir `PERSONA.md` â€” reduzir de ~1.800 chars para < 400 chars (essÃªncia apenas).
- [ ] **T05**: Integrar token-router no `telegram-bot.js` â€” substituir chamada direta ao OpenAI.
- [ ] **T06**: Adicionar contador de tokens no log â€” monitorar gasto real por mensagem.

---

## 5. Estimativa de Economia

| TÃ©cnica | Economia Estimada |
|---|---|
| Model Routing (simplesâ†’mini) | 50-70% |
| Semantic Cache (Redis) | 30-40% nas repetiÃ§Ãµes |
| Context Triangulation (RAG threshold) | 40-60% nos tokens de entrada |
| PERSONA.md comprimida | 15-25% |
| **Total combinado** | **~80-90%** |
